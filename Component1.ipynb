{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02_Component1_ABSA.ipynb\n",
   "id": "e5b00d541465598b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T14:52:38.008404Z",
     "start_time": "2025-06-08T14:52:37.428735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data + viz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from joblib import load, dump\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "# styling\n",
    "sns.set(style=\"whitegrid\")\n"
   ],
   "id": "3846dfb41a00a917",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mseaborn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# NLP\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'spacy'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Load spaCy model (for dependency parsing) and Transformers tokenizer/models\n",
    "\n"
   ],
   "id": "9cf36254dba9870c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# spaCy for rules\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Transformers for aspect extraction (token classification) and sentiment\n",
    "asp_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")  # placeholder; replace with your fine-tuned model\n",
    "asp_model     = AutoModelForTokenClassification.from_pretrained(\"path/to/fine-tuned-aspect-extractor\")\n",
    "\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "sent_model     = AutoModelForSequenceClassification.from_pretrained(\"path/to/fine-tuned-absa-sentiment\")\n"
   ],
   "id": "7a73e12861c0158f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load Preprocessed Data\n",
   "id": "9437d5b4d19a7f90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_excel(\"cleaned_feedback_preprocessed.xlsx\")\n",
    "# Identify the cleaned text column\n",
    "text_col = \"feedback_text_clean\"  # adjust if different\n",
    "print(\"Records:\", len(df))\n"
   ],
   "id": "6ff6a13341c56a88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Load Aspect Ontology\n",
   "id": "91cfa24901035307"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "with open(\"config/aspect_ontology.json\") as f:\n",
    "    ontology = json.load(f)\n",
    "# ontology = {\"mentorship\": [\"mentor\", \"supervisor\", ...], ...}\n",
    "aspect_categories = list(ontology.keys())\n"
   ],
   "id": "f7a9406ff90c9ba3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Aspect Term Extraction\n",
    "### 4.1 Rule-Based Extraction\n"
   ],
   "id": "26605844cde7d4c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rule_extract(text):\n",
    "    doc = nlp(text)\n",
    "    found = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"NOUN\",\"PROPN\") and token.lemma_ in sum(ontology.values(),[]):\n",
    "            found.append((token.lemma_, token.i))\n",
    "    return found\n"
   ],
   "id": "acaa9f809a10a029"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Transformer-Based Extraction\n",
   "id": "d3b798f290d5e52a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "asp_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=asp_model,\n",
    "    tokenizer=asp_tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def ner_extract(text):\n",
    "    ents = asp_pipe(text)\n",
    "    # ents: [{\"entity_group\":\"ASPECT\",\"word\":\"mentor\",\"start\":..,\"end\":..}, ...]\n",
    "    return [(e[\"word\"], e[\"start\"], e[\"end\"]) for e in ents]\n"
   ],
   "id": "99fc2b22312e73e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Hybrid Extraction\n",
   "id": "c851bfadd1f23664"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_aspects(text):\n",
    "    rules = rule_extract(text)\n",
    "    ner   = ner_extract(text)\n",
    "    # unify by term\n",
    "    terms = set([r[0] for r in rules] + [n[0].lower() for n in ner])\n",
    "    return list(terms)\n",
    "\n",
    "# example\n",
    "print(extract_aspects(df[text_col].iloc[0]))\n"
   ],
   "id": "d23941dbe3c148e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Category Detection & Disambiguation\n",
   "id": "2fd6188e861fb9ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def map_to_category(term):\n",
    "    # direct lexicon match\n",
    "    for cat, lex in ontology.items():\n",
    "        if term in lex:\n",
    "            return cat\n",
    "    # fuzzy match\n",
    "    for cat, lex in ontology.items():\n",
    "        if get_close_matches(term, lex, n=1, cutoff=0.8):\n",
    "            return cat\n",
    "    return None\n"
   ],
   "id": "50d2314dc116d9d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Sentiment Polarity Classification\n",
   "id": "fb4c91d4abf16fd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sent_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=sent_model,\n",
    "    tokenizer=sent_tokenizer,\n",
    "    function_to_apply=\"none\"  # ensure we get logits\n",
    ")\n",
    "\n",
    "def classify_sentiment(text, aspect_term):\n",
    "    # mark aspect in text\n",
    "    marked = text.replace(aspect_term, f\"[ASP]{aspect_term}[ASP]\")\n",
    "    pred = sent_pipe(marked)[0]\n",
    "    # pred: {\"label\":\"POSITIVE\",\"score\":0.98}\n",
    "    label = pred[\"label\"]\n",
    "    return {\"POSITIVE\":1, \"NEUTRAL\":0, \"NEGATIVE\":-1}[label]\n"
   ],
   "id": "2a5de24452a354ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Run ABSA Pipeline\n",
   "id": "d47f240743bea340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "records = []\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[text_col]\n",
    "    terms = extract_aspects(text)\n",
    "    for term in terms:\n",
    "        cat = map_to_category(term)\n",
    "        if not cat: continue\n",
    "        polarity = classify_sentiment(text, term)\n",
    "        records.append({\n",
    "            \"id\": idx,\n",
    "            \"term\": term,\n",
    "            \"category\": cat,\n",
    "            \"polarity\": polarity,\n",
    "            \"text\": text\n",
    "        })\n",
    "absa_df = pd.DataFrame(records)\n",
    "len(absa_df)\n"
   ],
   "id": "4a9f1ba8113ac7be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Evaluation on Annotated Test Set\n",
   "id": "e0c4f4cc8debb277"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# assuming you have true labels in absa_test.csv with columns: id, term, category, polarity\n",
    "test = pd.read_csv(\"data/absa_test.csv\")\n",
    "# merge predictions\n",
    "eval_df = test.merge(absa_df, on=[\"id\",\"term\"], suffixes=(\"_true\",\"_pred\"))\n",
    "print(classification_report(eval_df[\"category_true\"], eval_df[\"category_pred\"]))\n",
    "print(classification_report(eval_df[\"polarity_true\"], eval_df[\"polarity_pred\"]))\n"
   ],
   "id": "fa227f06f99e61e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Visualizations\n",
   "id": "f0ed2e3e3ea803f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 9.1 Aspect mention counts\n",
    "plt.figure(figsize=(8,4))\n",
    "absa_df[\"category\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Aspect Mention Counts\")\n",
    "plt.ylabel(\"Mentions\")\n",
    "plt.show()\n",
    "\n",
    "# 9.2 Sentiment distribution per aspect\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=absa_df, x=\"category\", hue=\"polarity\")\n",
    "plt.title(\"Sentiment by Aspect\")\n",
    "plt.xlabel(\"Aspect\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Polarity\", labels=[\"Neg\",\"Neu\",\"Pos\"])\n",
    "plt.show()\n"
   ],
   "id": "2e0cc3aa860e10bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Save Outputs & Models\n",
   "id": "598f5ba9d68e96bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "absa_df.to_excel(\"absa_aspect_sentiment.xlsx\", index=False)\n",
    "dump(ontology, \"models/aspect_ontology.joblib\")\n",
    "dump(asp_model, \"models/aspect_extractor_model.joblib\")\n",
    "dump(sent_model, \"models/sentiment_classifier_model.joblib\")\n"
   ],
   "id": "dea8d928d3bd0e3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
