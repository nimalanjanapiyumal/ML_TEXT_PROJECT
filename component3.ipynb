{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 04_Component3_Persona_Clustering.ipynb\n",
    "\n"
   ],
   "id": "3e5d6be01dad9a4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setup & Imports\n",
   "id": "15c50f672381275c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:25:34.868718Z",
     "start_time": "2025-06-15T06:25:29.265523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data manipulation & IO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP for skill‐keywords (if used)\n",
    "from joblib import load\n",
    "\n",
    "# Dimensionality reduction & clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Scaling & model persistence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ],
   "id": "1ac4a21f94811e97",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load Inputs\n",
   "id": "71d4de038c4dfc3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:29:57.932150Z",
     "start_time": "2025-06-15T06:29:57.725215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2.1 Load gap metrics from Notebook 2\n",
    "gap_df = pd.read_excel(\"gap_analysis_metrics.xlsx\")\n",
    "# columns: id, aspect, exp_mean_sent, exp2_mean_sent, semantic_gap, sentiment_gap, hybrid_gap, overall_satisfaction\n",
    "\n",
    "# 2.2 Load ABSA sentiment data from Notebook 2\n",
    "absa_df = pd.read_excel(\"absa_aspect_sentiment.xlsx\")\n",
    "# columns: id, term, category, polarity, text\n",
    "\n",
    "# 2.3 Optionally load cleaned raw for quotes or skills\n",
    "clean = pd.read_excel(\"cleaned_feedback_preprocessed.xlsx\")\n"
   ],
   "id": "ad06d726f5d05422",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Feature Engineering\n",
   "id": "260891a6f5342f6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:30:09.154199Z",
     "start_time": "2025-06-15T06:30:09.103378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3. Feature Engineering ---\n",
    "\n",
    "# 3.0 Reset indices to create a stable 'id' column in each DF\n",
    "absa_df = absa_df.reset_index(drop=True).reset_index().rename(columns={'index':'id'})\n",
    "gap_df  = gap_df.reset_index(drop=True).reset_index().rename(columns={'index':'id'})\n",
    "clean   = clean.reset_index(drop=True).reset_index().rename(columns={'index':'id'})\n",
    "\n",
    "# Verify uniqueness on the 'id' Series\n",
    "assert absa_df['id'].is_unique, \"absa_df id values are not unique!\"\n",
    "assert gap_df['id'].is_unique,  \"gap_df id values are not unique!\"\n",
    "assert clean['id'].is_unique,   \"clean id values are not unique!\"\n",
    "\n",
    "# 3.1 Pivot ABSA → average polarity per aspect per intern\n",
    "absa_pivot = (\n",
    "    absa_df\n",
    "      .groupby(['id','category'])['polarity']\n",
    "      .mean()\n",
    "      .unstack(fill_value=0)\n",
    "      .add_prefix('sent_')\n",
    ")\n",
    "\n",
    "# 3.2 Pivot gap_df → sentiment_gap, semantic_gap, hybrid_gap per aspect per intern\n",
    "gap_sent = (\n",
    "    gap_df\n",
    "      .pivot(index='id', columns='aspect', values='sentiment_gap')\n",
    "      .add_prefix('gap_sent_')\n",
    "      .fillna(0)\n",
    ")\n",
    "gap_sem = (\n",
    "    gap_df\n",
    "      .pivot(index='id', columns='aspect', values='semantic_gap')\n",
    "      .add_prefix('gap_sem_')\n",
    "      .fillna(0)\n",
    ")\n",
    "gap_hyb = (\n",
    "    gap_df\n",
    "      .pivot(index='id', columns='aspect', values='hybrid_gap')\n",
    "      .add_prefix('gap_hyb_')\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# 3.3 Aspect mention frequencies per intern\n",
    "freq = (\n",
    "    absa_df\n",
    "      .groupby(['id','category'])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    "      .add_prefix('freq_')\n",
    ")\n",
    "\n",
    "# 3.4 Skill‐keyword indicators\n",
    "skills = [\"python\",\"git\",\"agile\",\"docker\",\"aws\",\"java\",\"sql\",\"javascript\"]\n",
    "skill_flags = clean[['id']].copy()\n",
    "for skill in skills:\n",
    "    skill_flags[f\"skill_{skill}\"] = clean['feedback_text_clean'].apply(\n",
    "        lambda toks: int(isinstance(toks, list) and skill in toks)\n",
    "    )\n",
    "skill_flags = skill_flags.set_index('id')\n",
    "\n",
    "# 3.5 Merge all feature sets\n",
    "features = (\n",
    "    absa_pivot\n",
    "      .join([gap_sent, gap_sem, gap_hyb, freq, skill_flags], how='outer')\n",
    "      .fillna(0)\n",
    ")\n",
    "features.index.name = 'id'\n",
    "\n",
    "# 3.6 Add overall satisfaction if available\n",
    "if 'overall_satisfaction' in clean.columns:\n",
    "    overall = clean.set_index('id')['overall_satisfaction']\n",
    "    features = features.join(overall, how='left')\n",
    "\n",
    "print(\"Feature matrix shape:\", features.shape)\n",
    "features.head()\n"
   ],
   "id": "2bfe6965a36476e4",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'is_unique'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_30556\\544171498.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;31m# --- 3. Feature Engineering ---\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;31m# 3.0 Reset indices to create a stable 'id' column in each DF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mabsa_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mabsa_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'index'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;34m'id'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\ML_Project\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6295\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6296\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6297\u001B[0m         ):\n\u001B[0;32m   6298\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6299\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'is_unique'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Scaling & Dimensionality Reduction\n",
   "id": "4b14db2afffc692b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4.1 Separate features vs. labels\n",
    "X = features.drop(columns=['overall_satisfaction'], errors='ignore')\n",
    "ids = X.index\n",
    "\n",
    "# 4.2 Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save scaler for future use\n",
    "dump(scaler, \"models/persona_scaler.joblib\")\n",
    "\n",
    "# 4.3 PCA (for diagnostics)\n",
    "pca = PCA(n_components=0.90, random_state=42)  # keep 90% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"PCA reduces to {X_pca.shape[1]} dimensions\")\n",
    "\n",
    "# 4.4 t-SNE for 2D visualization\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n"
   ],
   "id": "15f244a53bfec02e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Visualize t-SNE embedding\n",
   "id": "b00e68690c723839"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], s=20, alpha=0.6)\n",
    "plt.title(\"t-SNE Projection of Intern Feature Vectors\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.show()\n"
   ],
   "id": "88202d0d0f92bbb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Clustering & Validation\n",
   "id": "e13b8225c3631506"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5.1 Determine optimal k with elbow and silhouette\n",
    "wcss, sil, ks = [], [], list(range(2,8))\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_scaled)\n",
    "    wcss.append(km.inertia_)\n",
    "    sil.append(silhouette_score(X_scaled, km.labels_))\n",
    "\n",
    "# 5.2 Plot\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "ax1.plot(ks, wcss, '-o', label='Inertia (WCSS)')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('WCSS')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(ks, sil, '-o', color='C1', label='Silhouette')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "fig.legend(loc='upper right')\n",
    "plt.title(\"Elbow & Silhouette for k-means\")\n",
    "plt.show()\n",
    "\n",
    "# 5.3 Choose k (e.g., k=4)\n",
    "k_opt = 4\n",
    "km = KMeans(n_clusters=k_opt, random_state=42, n_init=10).fit(X_scaled)\n",
    "labels = km.labels_\n",
    "features['persona'] = labels\n",
    "\n",
    "# Save model\n",
    "dump(km, \"models/persona_kmeans.joblib\")\n"
   ],
   "id": "f8638410d1348db8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Persona Visualization\n",
   "id": "b2e69669541f6621"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6.1 t-SNE colored by persona\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=labels, palette='tab10', s=30, alpha=0.7)\n",
    "plt.title(\"t-SNE of Interns Colored by Persona Cluster\")\n",
    "plt.legend(title=\"Persona\")\n",
    "plt.show()\n",
    "\n",
    "# 6.2 Silhouette for chosen k\n",
    "print(\"Silhouette Score (k=4):\", silhouette_score(X_scaled, labels))\n",
    "print(\"Davies–Bouldin Index (k=4):\", davies_bouldin_score(X_scaled, labels))\n"
   ],
   "id": "8cb21eb760d43efc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Persona Profiling & Export\n",
   "id": "840210aa6c4bcda0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7.1 Compute cluster centroids in original feature space\n",
    "centroids = pd.DataFrame(km.cluster_centers_, columns=X.columns)\n",
    "centroids = pd.DataFrame(scaler.inverse_transform(centroids), columns=X.columns)\n",
    "centroids['persona'] = range(k_opt)\n",
    "\n",
    "# 7.2 Representative quotes per persona\n",
    "reps = []\n",
    "for p in range(k_opt):\n",
    "    ids_p = features[features['persona']==p].index\n",
    "    # sample up to 3 quotes from raw feedback\n",
    "    quotes = clean.loc[ids_p, 'feedback_text']\\\n",
    "                 .dropna().sample(3, random_state=42).tolist()\n",
    "    reps.append({'persona':p, 'quotes':quotes})\n",
    "\n",
    "reps_df = pd.DataFrame(reps).set_index('persona')\n",
    "\n",
    "# 7.3 Combine into persona profiles\n",
    "persona_profiles = centroids.join(reps_df, on='persona')\n",
    "persona_profiles.to_excel(\"persona_profiles.xlsx\")\n",
    "\n",
    "# 7.4 Show summary\n",
    "persona_profiles.head()\n"
   ],
   "id": "e0ffc57fb39b4f04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Recommendations Placeholder\n",
   "id": "565c77e815f9c1ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "93db1a36aaa26ab4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
