{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 03_Component2_Gap_Analysis.ipynb\n",
   "id": "c47a73a182740b84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Environment Setup & Imports\n",
   "id": "81e0ffcba6b96584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP & embeddings\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from joblib import load, dump\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ],
   "id": "945f18c975984413"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load Data & Models\n",
   "id": "51b1bcc415917bdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2.1 Preprocessed feedback\n",
    "df = pd.read_excel(\"cleaned_feedback_preprocessed.xlsx\")\n",
    "\n",
    "# 2.2 Aspect ontology\n",
    "with open(\"config/aspect_ontology.json\") as f:\n",
    "    ontology = json.load(f)\n",
    "aspect_categories = list(ontology.keys())\n",
    "\n",
    "# 2.3 Sentence classifier (expectation vs. experience vs. other)\n",
    "sent_clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"path/to/fine-tuned-sentence-classifier\",   # e.g., RoBERTa fine-tuned\n",
    "    tokenizer=\"path/to/fine-tuned-sentence-classifier\",\n",
    "    return_all_scores=False\n",
    ")\n",
    "\n",
    "# 2.4 Sentiment classifier (reuse from Component 1)\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"path/to/fine-tuned-absa-sentiment\",\n",
    "    tokenizer=\"path/to/fine-tuned-absa-sentiment\"\n",
    ")\n",
    "\n",
    "# 2.5 Embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2.6 spaCy for sentence segmentation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2.7 Text preprocessing function (from Notebook 1)\n",
    "from joblib import load\n",
    "preprocess_text = load(\"models/preprocess_text.fn\")\n"
   ],
   "id": "4b5bf3fb05403566"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Helper Functions\n",
   "id": "6b27452441e685d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def segment_sentences(text):\n",
    "    \"\"\"Split raw text into sentences via spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "def classify_sentence_type(sent):\n",
    "    \"\"\"Return 'expect', 'experience', or 'other'.\"\"\"\n",
    "    # model returns label e.g. 'EXPECTATION', 'EXPERIENCE', 'OTHER'\n",
    "    lab = sent_clf(sent)[0]['label']\n",
    "    return lab.lower()  # e.g., 'expectation' -> 'expect'\n",
    "\n",
    "def map_sentence_to_aspects(sent):\n",
    "    \"\"\"\n",
    "    Return list of aspect categories that have any lexicon term in the sentence.\n",
    "    Simplest: look for any ontology term lemma in the preprocessed tokens.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(sent)\n",
    "    cats = set()\n",
    "    for cat, lex in ontology.items():\n",
    "        for term in lex:\n",
    "            if term in tokens:\n",
    "                cats.add(cat)\n",
    "                break\n",
    "    return list(cats)\n"
   ],
   "id": "f31d1576318b10df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Build Expectation & Experience Sets\n",
   "id": "906b35f186f0def9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare storage\n",
    "records = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['feedback_text_clean']  # your cleaned free-text field\n",
    "    sents = segment_sentences(text)\n",
    "    # classify and tag each sentence\n",
    "    for sent in sents:\n",
    "        typ = classify_sentence_type(sent)\n",
    "        aspects = map_sentence_to_aspects(sent)\n",
    "        if not aspects:\n",
    "            continue\n",
    "        # sentiment of the sentence (âˆ’1,0,+1)\n",
    "        sentiment = sentiment_pipe(sent)[0]\n",
    "        score = {'NEGATIVE':-1,'NEUTRAL':0,'POSITIVE':1}[sentiment['label']]\n",
    "        records.append({\n",
    "            'id': idx,\n",
    "            'sentence': sent,\n",
    "            'type': typ,            # 'expect', 'experience', or 'other'\n",
    "            'aspects': aspects,     # list of mapped aspect categories\n",
    "            'sentiment': score\n",
    "        })\n",
    "\n",
    "gap_df = pd.DataFrame(records)\n",
    "gap_df.head()\n"
   ],
   "id": "4272c56335219625"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Compute Per-Aspect Metrics per Intern\n",
   "id": "3f47ac8491b2e64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# initialize result storage\n",
    "res = []\n",
    "\n",
    "alpha = 0.6  # balance parameter, tuned earlier\n",
    "\n",
    "for idx, group in gap_df.groupby('id'):\n",
    "    # overall satisfaction if available\n",
    "    overall = df.loc[idx, 'overall_satisfaction'] if 'overall_satisfaction' in df.columns else np.nan\n",
    "\n",
    "    # for each aspect:\n",
    "    for aspect in aspect_categories:\n",
    "        sub = group[group['aspects'].apply(lambda L: aspect in L)]\n",
    "        exp_sents = sub[sub['type']=='expect']['sentence'].tolist()\n",
    "        exp_scores = sub[sub['type']=='expect']['sentiment'].tolist()\n",
    "        exp_embeds = embedder.encode(exp_sents) if exp_sents else np.zeros((1,768))\n",
    "\n",
    "        exp_mean_sent = np.mean(exp_scores) if exp_scores else 0.0\n",
    "        exp_centroid = exp_embeds.mean(axis=0) if len(exp_sents)>0 else np.zeros((768,))\n",
    "\n",
    "        exp_sents2 = sub[sub['type']=='experience']['sentence'].tolist()\n",
    "        exp2_scores = sub[sub['type']=='experience']['sentiment'].tolist()\n",
    "        exp2_embeds = embedder.encode(exp_sents2) if exp_sents2 else np.zeros((1,768))\n",
    "\n",
    "        exp2_mean_sent = np.mean(exp2_scores) if exp2_scores else 0.0\n",
    "        exp2_centroid = exp2_embeds.mean(axis=0) if len(exp_sents2)>0 else np.zeros((768,))\n",
    "\n",
    "        # semantic gap\n",
    "        sim = cosine_similarity(\n",
    "            exp_centroid.reshape(1,-1),\n",
    "            exp2_centroid.reshape(1,-1)\n",
    "        )[0,0]\n",
    "        sem_gap = 1 - sim\n",
    "\n",
    "        # sentiment gap\n",
    "        sent_gap = exp2_mean_sent - exp_mean_sent\n",
    "\n",
    "        # hybrid gap\n",
    "        hyb_gap = alpha*sem_gap + (1-alpha)*abs(sent_gap)\n",
    "\n",
    "        res.append({\n",
    "            'id': idx,\n",
    "            'aspect': aspect,\n",
    "            'exp_mean_sent': exp_mean_sent,\n",
    "            'exp2_mean_sent': exp2_mean_sent,\n",
    "            'semantic_gap': sem_gap,\n",
    "            'sentiment_gap': sent_gap,\n",
    "            'hybrid_gap': hyb_gap,\n",
    "            'overall_satisfaction': overall\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(res)\n",
    "metrics_df.head()\n"
   ],
   "id": "b59a7075f652c859"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Visualizations\n",
   "id": "ad14b03055338756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6.1 Sentence type distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=gap_df, x='type')\n",
    "plt.title(\"Sentence Type Distribution\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 6.2 Hybrid gap boxplot per aspect\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=metrics_df, x='aspect', y='hybrid_gap')\n",
    "plt.title(\"Hybrid Gap per Aspect\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 6.3 Correlation between overall satisfaction and hybrid gaps\n",
    "corr = metrics_df[['overall_satisfaction','hybrid_gap']].corr().iloc[0,1]\n",
    "print(f\"Pearson r = {corr:.2f}\")\n",
    "sns.scatterplot(data=metrics_df, x='hybrid_gap', y='overall_satisfaction', alpha=0.3)\n",
    "plt.title(\"Overall Satisfaction vs. Hybrid Gap\")\n",
    "plt.show()\n"
   ],
   "id": "c0db7d57e67e1e00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Save Metrics & Models\n",
   "id": "670278e869aedbb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save results\n",
    "metrics_df.to_excel(\"gap_analysis_metrics.xlsx\", index=False)\n",
    "# (optional) save alpha for later\n",
    "dump(alpha, \"models/hybrid_gap_alpha.joblib\")\n"
   ],
   "id": "633953e69bd55e08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab70ec0efc42b71f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
